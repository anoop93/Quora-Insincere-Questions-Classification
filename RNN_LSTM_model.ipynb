{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import tensorflow as tf\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "# import dask.dataframe as dd\n",
    "# import dask.multiprocessing\n",
    "# import dask.threaded\n",
    "# import dask\n",
    "# import timeit\n",
    "# from time import time\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import spacy\n",
    "# nlp = spacy.load('en')\n",
    "# tqdm.pandas()\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Dropout, Activation, CuDNNGRU, CuDNNLSTM, Layer\n",
    "from tensorflow.keras.layers import Bidirectional, Flatten, SpatialDropout1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers, layers\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "19cfde1b6c8a4181ac364f30d7592beaa4ba5145"
   },
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_df['target'].values),\n",
    "                                                 train_df['target'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "a537920dd57c04b8a8b8ed598fbb80e59466e57e"
   },
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "#     x = x.lower()\n",
    "    x = x.replace(\"'\",\"'\")\n",
    "    x = ' '.join(word_tokenize(x))\n",
    "    return str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "2e49a1c99ba73878f08f3380fc1da2487e238fe6"
   },
   "outputs": [],
   "source": [
    "#restore\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].str.lower().apply(tokenize)\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].str.lower().apply(tokenize)\n",
    "# sentences = train_df[\"question_text\"].apply(lambda x: x.split()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "ec5e854306b1a82b59f2ebe3cfcdd086683d1fc3"
   },
   "outputs": [],
   "source": [
    "#restore\n",
    "mispell_dict = {' .net':' dotnet',\n",
    "                ' ^2':' square',\n",
    "                'bhakts':'followers',\n",
    "#                 'l&t':'larsen & toubro',\n",
    "                '/math':'math',\n",
    "                'demonetisation':'demonetization',\n",
    "                'cryptocurrencies':'cryptocurrency',\n",
    "                'quorans':'quoran',\n",
    "                ' an':' ',\n",
    "                ' a ':' ',\n",
    "                ' the ':' ',\n",
    "                ' this ':' ',\n",
    "                ' that ':' ',\n",
    "                ' then ':' ',\n",
    "                ' ..': '',\n",
    "                '+':'',\n",
    "                '\\\\': '',\n",
    "                '^': '',\n",
    "                '“':'\"',\n",
    "                '”':'\"',\n",
    "                '？':'?',\n",
    "                '£':'pound',\n",
    "                '€':'euro',\n",
    "                \"pokémon\": \"pokemon\",\n",
    "                '…':'',\n",
    "                '—':'-',\n",
    "                '°':'',\n",
    "                'non':'non'\n",
    "}\n",
    "# mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    text = text.replace('’', \"'\")\n",
    "    text = text.replace('‘', \"'\")\n",
    "    for k,v in mispell_dict.items():\n",
    "        if k in text:\n",
    "            text = text.replace(k,v)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "13a274fb0ca5f3b22ca48c849008476a729c8a62"
   },
   "outputs": [],
   "source": [
    "#restore\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].apply(replace_typical_misspell)\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].apply(replace_typical_misspell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "81cbea7689e941a544086589dc1590a70eb364e9"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "    \n",
    "\n",
    "def f1(y_true, y_pred, plot=False):\n",
    "    '''\n",
    "    metric from here \n",
    "    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "    '''\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "bc8cef7a1f06d4f04d992720674204d2a4ad1f47"
   },
   "outputs": [],
   "source": [
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 40000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 50 # max number of words in a question to use\n",
    "\n",
    "def load_data_kfold(k):\n",
    "    train_X = train_df[\"question_text\"].values\n",
    "    train_y = train_df[\"target\"].values\n",
    "    test_X = test_df[\"question_text\"].values\n",
    "    \n",
    "    folds = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=1).split(train_X, train_y))\n",
    "    \n",
    "    #tokenize\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    #padding\n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    return folds, train_X, train_y, test_X, tokenizer\n",
    "\n",
    "k = 5\n",
    "folds, train_X, train_y, test_X, tokenizer = load_data_kfold(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "47cc1955c4eb0ea77ba016e90819d46e4ec70180"
   },
   "outputs": [],
   "source": [
    "del train_df\n",
    "import gc; gc.collect()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fcf9a8be45194f37e10975bc1dea9f45e441501a"
   },
   "source": [
    "We have four different types of embeddings.\n",
    " * GoogleNews-vectors-negative300 - https://code.google.com/archive/p/word2vec/\n",
    " * glove.840B.300d - https://nlp.stanford.edu/projects/glove/\n",
    " * paragram_300_sl999 - https://cogcomp.org/page/resource_view/106\n",
    " * wiki-news-300d-1M - https://fasttext.cc/docs/en/english-vectors.html\n",
    " \n",
    " A very good explanation for different types of embeddings are given in this [kernel](https://www.kaggle.com/sbongo/do-pretrained-embeddings-give-you-the-extra-edge). Please refer the same for more details..\n",
    "\n",
    "**Glove Embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "23f130e80159bb1701e449e2e91199dbfff1f1d4"
   },
   "outputs": [],
   "source": [
    "#restore\n",
    "EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix_1 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None: embedding_matrix_1[i] = embedding_vector\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_1[i] = embedding_vector\n",
    "    elif embeddings_index.get(word.capitalize()) is not None:\n",
    "        embedding_matrix_1[i] = embeddings_index.get(word.capitalize())\n",
    "    elif embeddings_index.get(word.upper()) is not None:\n",
    "        embedding_matrix_1[i] = embeddings_index.get(word.upper())\n",
    "\n",
    "del word_index, embeddings_index, all_embs\n",
    "import gc; gc.collect()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83d6dca6b7cf94158a8645f8aaf0c849ad15176c"
   },
   "source": [
    "**Wiki News FastText Embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "13abc44488dfc9f046ffb7f43d6995a94650cdb3"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix_2 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None: embedding_matrix_2[i] = embedding_vector\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_2[i] = embedding_vector\n",
    "    elif embeddings_index.get(word.capitalize()) is not None:\n",
    "        embedding_matrix_2[i] = embeddings_index.get(word.capitalize())\n",
    "    elif embeddings_index.get(word.upper()) is not None:\n",
    "        embedding_matrix_2[i] = embeddings_index.get(word.upper())\n",
    "        \n",
    "del word_index, embeddings_index, all_embs\n",
    "import gc; gc.collect()\n",
    "time.sleep(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "55b609fc3862591b5f5ba14519f4c13168fe964c"
   },
   "source": [
    "**Paragram Embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "885aee337076f6dfd08e1213d45c668c9e1c5a77"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix_3 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None: embedding_matrix_3[i] = embedding_vector\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_3[i] = embedding_vector\n",
    "    elif embeddings_index.get(word.capitalize()) is not None:\n",
    "        embedding_matrix_3[i] = embeddings_index.get(word.capitalize())\n",
    "    elif embeddings_index.get(word.upper()) is not None:\n",
    "        embedding_matrix_3[i] = embeddings_index.get(word.upper())\n",
    "\n",
    "del word_index, embeddings_index, all_embs\n",
    "import gc; gc.collect()\n",
    "time.sleep(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe0ea934fff639a05e6e080bdd5e61ea3e198d50"
   },
   "source": [
    "**Word2vec Embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "81a19458d336ac1322dde9a3b379fb36423f2910"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "EMBEDDING_FILE = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix_4 = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    if word in embeddings_index:\n",
    "        embedding_vector = embeddings_index.get_vector(word)\n",
    "        embedding_matrix_4[i] = embedding_vector\n",
    "    elif word.capitalize() in embeddings_index:\n",
    "        embedding_matrix_4[i] = embeddings_index.get_vector(word.capitalize())\n",
    "    elif word.upper() in embeddings_index:\n",
    "        embedding_matrix_4[i] = embeddings_index.get_vector(word.upper())\n",
    "        \n",
    "del word_index, embeddings_index\n",
    "import gc; gc.collect()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "806f5468512dccf03d47a6fbd339c5c743cfb0d7"
   },
   "source": [
    "**Combine :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "1a4c19ffc07198827e9131f192bd9e61ba3b81fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate((embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4), axis=1)  \n",
    "del embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "7a9e03be964da76cddacaa5fff5ef132ce4eeee3"
   },
   "outputs": [],
   "source": [
    "def best_f1(y_true, y_pred, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32f10b0a5ba02d0470c32a67d51fecf84cbabf39"
   },
   "source": [
    "**convolution model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec79fa3f0ce3c35d42df3cb7c87b1a1460d10452"
   },
   "source": [
    "filter_sizes = [1,2,3,5]\n",
    "num_filters = 36\n",
    "from keras.layers import Conv1D, MaxPool1D, BatchNormalization\n",
    "import keras.backend as K\n",
    "def get_cmodel():    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "#     x = Lambda(lambda x: K.reverse(x,axes=-1))(inp)\n",
    "    x = Embedding(max_features, embed_size * 4, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    #x = Reshape((maxlen, embed_size, 1))(x)\n",
    "    \n",
    "    conv_0 = Conv1D(num_filters, kernel_size=(filter_sizes[0], embed_size * 4),\n",
    "                                 kernel_initializer='he_normal', activation='elu')(x)\n",
    "    conv_1 = Conv1D(num_filters, kernel_size=(filter_sizes[1], embed_size * 4),\n",
    "                                 kernel_initializer='he_normal', activation='elu')(x)\n",
    "    conv_2 = Conv1D(num_filters, kernel_size=(filter_sizes[2], embed_size * 4), \n",
    "                                 kernel_initializer='he_normal', activation='elu')(x)\n",
    "    conv_3 = Conv1D(num_filters, kernel_size=(filter_sizes[3], embed_size * 4),\n",
    "                                 kernel_initializer='he_normal', activation='elu')(x)\n",
    "    \n",
    "    maxpool_0 = MaxPool1D(pool_size=(maxlen - filter_sizes[0] + 1))(conv_0)\n",
    "    maxpool_1 = MaxPool1D(pool_size=(maxlen - filter_sizes[1] + 1))(conv_1)\n",
    "    maxpool_2 = MaxPool1D(pool_size=(maxlen - filter_sizes[2] + 1))(conv_2)\n",
    "    maxpool_3 = MaxPool1D(pool_size=(maxlen - filter_sizes[3] + 1))(conv_3)\n",
    "        \n",
    "    z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])   \n",
    "    z = Flatten()(z)\n",
    "    z = BatchNormalization()(z)\n",
    "        \n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73eddd13bf5b13b628d0c80341a577e6a93130d4"
   },
   "source": [
    "**capsule model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "8624028cb3b2dfa14ac228797efd1f3706dbc4aa"
   },
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, int(input_dim_capsule),\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "d16b1c60d83b406b25b6876426f694e64e6b8887"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size* 4, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "#     x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    x = Capsule(num_capsule=30, dim_capsule=20, routings=4, share_weights=True)(x)\n",
    "#     x = GlobalAveragePooling1D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    return Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "c03c8128a363023e8f63d16b108de007f78233c2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = get_model()\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "80e2500e02e5051d7c0ed02daa07376bb7b32fae"
   },
   "outputs": [],
   "source": [
    "pred_test_y = []\n",
    "pred_val_score = []\n",
    "pred_val_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "bead25d195dc6e127929fadd1e2c7d4655b0e27f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/1\n",
      "1044897/1044897 [==============================] - 342s 327us/step - loss: 0.1135 - acc: 0.9555 - val_loss: 0.1009 - val_acc: 0.9594\n",
      "261225/261225 [==============================] - 27s 104us/step\n",
      " 1024/56370 [..............................] - ETA: 5s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56370/56370 [==============================] - 6s 105us/step\n",
      "[{'threshold': 0.28004756569862366, 'f1': 0.6705398894714468}]\n",
      "*****************************************\n"
     ]
    }
   ],
   "source": [
    "# for i in range(4):\n",
    "for j, (train_idx, val_idx) in enumerate(folds):\n",
    "    if j > 0:\n",
    "        break\n",
    "    print('\\nFold ',j)\n",
    "    X_train_cv = train_X[train_idx]\n",
    "    y_train_cv = train_y[train_idx]\n",
    "    X_valid_cv = train_X[val_idx]\n",
    "    y_valid_cv = train_y[val_idx]\n",
    "    model = get_model()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.002), metrics=['accuracy'])\n",
    "#     if \"model1_\"+str(j)+\".h5\" in os.listdir('.') and i != 0:\n",
    "#         model.load_weights(\"model1_\"+str(j)+\".h5\")\n",
    "    model.fit(X_train_cv, y_train_cv, batch_size=512, epochs=1, validation_data=(X_valid_cv, y_valid_cv), class_weight=class_weights)\n",
    "#     model.save_weights(\"model1_\"+str(j)+\".h5\")\n",
    "    predict_val_y = model.predict([X_valid_cv], batch_size=1024, verbose=1)\n",
    "    best = best_f1(y_valid_cv, predict_val_y)\n",
    "    pred_val_score.append(best)\n",
    "    predict_test_y = model.predict([test_X], batch_size=1024, verbose=1)\n",
    "    pred_test_y.append(predict_test_y)\n",
    "    print(pred_val_score)\n",
    "    print('*****************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "909937b464b34ee5271396d2bb5e990a722b8d6a"
   },
   "outputs": [],
   "source": [
    "thres = [i['threshold'] for i in pred_val_score]\n",
    "test_y = [i for i in pred_test_y]\n",
    "p_test_y = np.zeros_like(np.array(test_y))\n",
    "for i in range(5):\n",
    "    p_test_y[i] = (test_y[i]>thres[i]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "a374922a48cecee8c94d03500813e92958b1114e"
   },
   "outputs": [],
   "source": [
    "p = (p_test_y.reshape((5, test_df.shape[0])).sum(axis=0)> 2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c7edb81b219606461fcd80e86217c1310fdb938"
   },
   "source": [
    "#restore\n",
    "#model.save_weights(\"model.h5\")\n",
    "model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c9a3b3dda5d6fa09c03b06b02d654f4a9b9f4a1"
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "out_df['prediction'] = p\n",
    "out_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
